# Product-Specific RAG Chatbot System

## Overview

This system enables users or developers to upload product information (as `.txt` files) for different products, which are then processed and indexed for retrieval-augmented generation (RAG) chat interactions. The system leverages Azure Blob Storage for file storage, PostgreSQL for metadata management, FAISS for similarity search, and an LLM (Ollama) for generating product-specific answers.

---

## System Purpose

- **Centralize product information**: Each product has its own knowledge base, uploaded and managed via API.
- **Enable product-specific chat**: Users can interact with a chatbot that answers questions using only the uploaded product data.
- **Automate data processing**: Uploaded files are automatically chunked, embedded, and indexed for fast retrieval.

---

## System Flow

```
+--------------------------+
| 1. Upload Product Info   |
|--------------------------|
| User/Developer uploads a |
| .txt file via API (/add  |
| -product or /update).    |
| Includes:                |
|  - Product Name          |
|  - API Key               |
+--------------------------+
             |
             v
+-----------------------------+
| 2. Save to Azure Storage    |
|-----------------------------|
| The .txt file is stored in  |
| Azure Blob under:           |
| bulipe-rag-data/{product}/  |
|   product_info.txt          |
+-----------------------------+
             |
             v
+------------------------------+
| 3. Save Metadata to DB       |
|------------------------------|
| Product name, API key, and   |
| blob path saved to PostgreSQL|
| for future reference.        |
+------------------------------+
             |
             v
+-----------------------------+
| 4. Trigger CI/CD Pipeline   |
|-----------------------------|
| GitHub Actions or Azure CI  |
| detects the new/updated file|
| and starts a job to process |
| it.                         |
+-----------------------------+
             |
             v
+-----------------------------+
| 5. Download from Azure      |
|-----------------------------|
| Pipeline fetches .txt from  |
| blob storage for processing.|
+-----------------------------+
             |
             v
+------------------------------+
| 6. Process with RAG Pipeline |
|------------------------------|
| The pipeline runs:           |
| - Text chunking              |
| - Embedding generation       |
| - FAISS index creation       |
+------------------------------+
             |
             v
+---------------------------------+
| 7. Save Outputs to Azure        |
|---------------------------------|
| Save generated files:           |
| - chunks.pkl                    |
| - faiss_store/index.faiss       |
| Back to same product folder in  |
| Azure Blob Storage              |
+---------------------------------+
             |
             v
+-------------------------------+
| 8. User Sends Chat Message    |
|-------------------------------|
| User sends a message via the  |
| chat interface with API key.  |
+-------------------------------+
             |
             v
+--------------------------------+
| 9. Load Product-Specific Data  |
|--------------------------------|
| Chat server uses API key to:   |
| - Identify the product         |
| - Load its FAISS and chunk.pkl |
+--------------------------------+
             |
             v
+----------------------------------+
| 10. Search & Generate Response   |
|----------------------------------|
| - Run similarity search using    |
|   FAISS                          |
| - Use result as context for LLM  |
| - Generate answer with Ollama    |
+----------------------------------+
             |
             v
+-------------------------------+
| 11. Send Response to User     |
|-------------------------------|
| The chatbot replies using only |
| product-specific data.         |
+-------------------------------+
```

---

## LLM Integration

- **Model Used:** Llama 3.2
- **Serving Platform:** Ollama (local LLM server)
- **Integration:**
  - The chat server sends the user query and retrieved product-specific context to the Ollama API endpoint (`http://localhost:11434/api/generate`).
  - Ollama runs the Llama 3.2 model to generate a response, strictly using the provided context.
  - The response is returned to the user via the chat API.
- **Purpose:**
  - Ensures that all chatbot answers are generated by a state-of-the-art LLM, but are strictly limited to the uploaded product data for accuracy and compliance.

---

## API Endpoints

### 1. Upload Product Info

- **Endpoint:** `POST /upload-file`
- **File:** `upload_file_azure.py`
- **Purpose:** Upload a new product info `.txt` file.
- **Request Body:**
  - `file_path`: Path to the `.txt` file.
  - `file_name`: Product name (used as folder).
  - `file_key`: Unique API key for the product.
- **Behavior:** 
  - Uploads the file to Azure Blob Storage under `{product}/product_info.txt`.
  - Saves metadata (product name, key, blob path) to PostgreSQL.

### 2. Update Product Info

- **Endpoint:** `PUT /update-file`
- **File:** `update_file_azure.py`
- **Purpose:** Update an existing product info file.
- **Request Body:**
  - `file_path`, `file_name`, `file_key` (as above)
- **Behavior:** 
  - Overwrites the file in Azure Blob Storage.
  - Updates metadata in PostgreSQL.

### 3. Delete Product Info

- **Endpoint:** `DELETE /delete-file`
- **File:** `delete_file_azure.py`
- **Purpose:** Delete a product info file and its metadata.
- **Request Body:**
  - `file_name`, `file_key`
- **Behavior:** 
  - Deletes the file from Azure Blob Storage.
  - Removes metadata from PostgreSQL.

### 4. Chat with Product Bot

- **Endpoint:** `POST /chat`
- **File:** `chat_server.py`
- **Purpose:** Send a chat message and get a product-specific answer.
- **Request Body:**
  - `message`: User's question.
  - `api_key`: Product API key (or via `X-API-Key` header).
- **Behavior:** 
  - Identifies the product using the API key.
  - Loads the product's FAISS index and chunks.
  - Runs similarity search and generates a response using the LLM.

---

## File Storage

- **Azure Blob Storage**: All product info files are stored under `bulipe-rag-data/{product}/product_info.txt`.
- **Processed Data**: After processing, each product has:
  - `processed_data/{product}/chunks.pkl` (pickled text chunks and embeddings)
  - `processed_data/{product}/faiss_store/index.faiss` (FAISS index for similarity search)

---

## Metadata Management

- **PostgreSQL Table:** `file_metadata`
- **Schema:**
  - `id`: Primary key
  - `file_name`: Product name
  - `file_key`: Unique API key
  - `source_path`: Original file path
  - `blob_path` or `local_path`: Path in Azure/local storage
  - `created_at`: Timestamp

- **Setup:** See `create_database.sql` for schema and comments.

---

## RAG Pipeline

- **Trigger:** CI/CD pipeline (e.g., GitHub Actions, Azure CI) detects new/updated files and runs the pipeline.
- **Processing Steps:** (see `process_pipeline.py`)
  1. **Download** the `.txt` file from Azure Blob Storage.
  2. **Chunk** the text (default: 10,000 chars, 500 overlap).
  3. **Generate Embeddings** using `sentence-transformers/all-MiniLM-L6-v2`.
  4. **Create FAISS Index** for fast similarity search.
  5. **Save** `chunks.pkl` and `index.faiss` back to Azure Blob Storage.

---

## Chatbot Query Flow

1. **User sends a message** with an API key.
2. **Chat server** (`chat_server.py`) identifies the product using the key.
3. **Loads** the product's FAISS index and chunks.
4. **Runs similarity search** to find relevant chunks.
5. **Sends context** to the LLM (Ollama) to generate a product-specific answer.
6. **Returns** the answer to the user.

---

## Directory Structure

```
.
├── chat_server.py
├── upload_file_azure.py
├── update_file_azure.py
├── delete_file_azure.py
├── process_pipeline.py
├── requirements.txt
├── create_database.sql
├── local_storage/
│   └── {product}/product_info.txt
├── processed_data/
│   └── {product}/
│       ├── chunks.pkl
│       └── faiss_store/index.faiss
└── ...
```

---

## Environment Setup

1. **Install dependencies:**
   ```
   pip install -r requirements.txt
   ```
2. **Set up PostgreSQL:**
   - Run `create_database.sql` to create the database and table.
3. **Configure Azure Blob Storage:**
   - Set your Azure connection string and container name in the relevant files.
4. **Run API servers:**
   - Start the upload/update/delete servers (FastAPI/Uvicorn).
   - Start the chat server (`chat_server.py`).
5. **Set up CI/CD:**
   - Configure your pipeline to run `process_pipeline.py` on new/updated files.

---

## Key Files

- `chat_server.py`: Main chat API, handles product-specific queries.
- `upload_file_azure.py`, `update_file_azure.py`, `delete_file_azure.py`: File management APIs.
- `process_pipeline.py`: RAG pipeline for chunking, embedding, and indexing.
- `requirements.txt`: Python dependencies.
- `create_database.sql`: Database schema.

---

## FAQ

**Q: How do I add a new product?**  
A: Use the `/upload-file` endpoint with a new product name and API key.

**Q: How does the chatbot know which product to use?**  
A: The API key in the chat request maps to a product in the database.

**Q: Can I update or delete product info?**  
A: Yes, use `/update-file` or `/delete-file` with the correct product name and API key.

**Q: How is my data secured?**  
A: Files are stored in Azure Blob Storage, and access is controlled via unique API keys.

---

## Contributing

- Follow the structure and flow described above.
- Document any new endpoints or changes to the pipeline.
- Ensure all new product info files are processed and indexed via the pipeline.

---


**For any questions, please refer to this README before reaching out. This document is designed to be your single source of truth for understanding and working with the system.**

--- 